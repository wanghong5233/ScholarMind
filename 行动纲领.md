# ScholarMind 项目行动纲领

**项目核心定位：** 不仅仅是一个“文档问答”工具，而是旨在成为一个**能够深度理解多模态学术论文**、**支持多文档上下文**、并**辅助研究人员进行文献综述、思路启发和论文草稿撰写**的**高级科研助理**。

---

## 第零阶段：项目净化与品牌重塑 (Phase 0: Purification & Rebranding)

**目标：** 彻底抹去旧项目的痕跡，注入 `ScholarMind` 的灵魂，让整个项目从内到外都成为您自己的作品。这是专业开发的第一步。

**核心价值：** 向面试官展示您专业的工程素养、对项目所有权的严肃态度和优秀的文档能力。

### 关键任务 (Tasks)

1.  **全面重命名 (Branding):**
    *   **描述**: 审视整个代码库，将所有与旧项目相关的命名（例如 `docker-compose.yml` 中的服务名 `swxy_api`、配置文件中的变量、代码注释、日志输出等）全部替换为 `scholarmind` 或其他相关的中性名称。
    *   **细化要点**:
        *   统一 `docker-compose.yml` 服务与网络命名：`scholarmind_api`、`scholarmind_db`、`scholarmind_vector`。
        *   统一 `.env` 键名前缀：例如 `SM_`（如 `SM_RAG_TOPK`, `SM_EMBEDDER_TYPE`）。
        *   替换代码中残留的产品文案、接口前缀、日志Tag，确保品牌一致性。
    *   **产出物**: 一个干净的、完全属于 `ScholarMind` 品牌的代码库。

2.  **代码清理 (Code Cleanup):**
    *   **描述**:
        *   删除旧项目中任何与金融研报场景强相关的、在新场景下无用的代码（如特定的解析规则、API端点）。
        *   清理掉所有废弃的、被注释掉的代码块，保持代码的整洁。
    *   **产出物**: 一个更轻量、更专注、可读性更高的代码库。

3.  **项目文档初始化 (Documentation):**
    *   **描述**: 精心编写第一版 `README.md`，内容应包括：
        *   项目Logo与徽章 (Badges)。
        *   清晰的项目定位和一句话介绍。
        *   核心功能列表 (Features)。
        *   包含 `docker-compose up -d` 的快速启动指南。
        *   明确的技术选型 (Tech Stack)。
        *   一个高级别的开发路线图 (Roadmap)，链接到本计划。
    *   **产出物**: 一个专业、信息全面的 `README.md` 文件。

4.  **环境与开源合规初始化 (Env & OSS readiness):**
    *   **描述**:
        *   添加 `.env.example`，覆盖 Embedding/Reranker/LLM 的可切换配置占位（见Phase 1“配置中心化”）。
        *   明确许可证 `LICENSE`（推荐 MIT）并在 `README.md` 标注。
        *   初始化 `CONTRIBUTING.md`、`CODE_OF_CONDUCT.md`（可简化模板）。
    *   **产出物**: `.env.example`、`LICENSE`、`CONTRIBUTING.md` 草案。

---

## 第一阶段：架构重构与基础加固 (Phase 1: Architectural Refactoring)

**目标：** 解决原项目的核心架构缺陷（硬编码、高耦合），搭建一个“高内聚、低耦合”、可扩展、可维护的“可插拔”系统。

**核心价值：** 这是整个项目中技术含金量最高的部分之一。它能集中体现您的软件设计能力、架构思维和对生产环境的理解（成本、安全、性能）。

### 关键任务：分步实施计划

我们将 Phase 1 分为四个逻辑部分，共八个可执行的子任务，确保每一步都建立在坚实的基础之上。

---

#### 第一部分：环境与规范 (Environment & Standards)

*本部分旨在为项目搭建生产级的“基础设施”，确保后续开发过程中的稳定性和可维护性。*

**任务1: 实施结构化日志 (Structured Logging)**
*   **目标**: 放弃传统 `print` 或简单 `logging`，引入 `loguru` 库，建立全局统一、可追踪、可分析的日志系统。
*   **关键步骤**:
    1.  配置 `loguru` 将日志输出为 JSON 格式。
    2.  创建 FastAPI 中间件，为每个请求生成唯一的 `request_id`，并将其注入所有后续日志中。
*   **产出物**: `utils/get_logger.py` 模块；FastAPI 日志中间件；结构化的日志输出。

**任务2: 建立标准化异常处理 (Standardized Exception Handling)**
*   **目标**: 构建健壮的全局异常处理机制，向前端返回统一、清晰的错误信息，同时记录详细的错误日志。
*   **关键步骤**:
    1.  创建 `exceptions` 目录，定义业务相关的自定义异常类（如 `ModelNotFoundError`, `VectorStoreError`）。
    2.  在 `app_main.py` 中，使用 FastAPI 的 `exception_handlers` 机制，注册全局异常处理器。
*   **产出物**: 自定义异常类；全局异常处理中间件；标准化的 JSON 错误响应格式。

---

#### 第二部分：核心抽象层 (Core Abstraction Layer)

*本部分是整个“可插拔”架构的核心，我们将用代码定义系统的标准“接口”和“数据语言”。*

**任务3: 定义核心数据模型与组件接口 (Define Core Data Models & Component Interfaces)**
*   **目标**: 彻底解耦数据流和功能模块，为后续所有功能（多模态、高级检索等）奠定基础。
*   **关键步骤**:
    1.  **数据模型**: 在 `schemas` 包下创建 `rag.py`，使用 `Pydantic` 定义 `Document` 和 `Chunk` 等核心数据结构。
    2.  **组件接口**: 创建 `service/core/abstractions` 目录，用于存放所有核心抽象基类（接口）。在其中为 `embedder.py`, `reranker.py`, `llm.py`, `vector_store.py` 分别定义 `BaseEmbedder`, `BaseReranker`, `BaseLLM`, `BaseVectorStore` 接口。
    3.  **接口设计**: 为接口设计统一、面向未来的方法签名，例如为 `BaseLLM` 预留 `stream_generate` 和 `generate_with_tools` 方法。
*   **产出物**: `schemas/rag.py`；`service/core/abstractions` 目录及其内部的接口定义文件。

---

#### 第三部分：配置与实现 (Configuration & Implementation)

*本部分旨在将第二部分的抽象接口“实例化”，并提供一个灵活的“开关”来动态选择使用哪个实例。*

**任务4: 配置中心化与组件工厂 (Centralize Configuration & Build Component Factory)**
*   **目标**: 将所有可变配置（模型类型、路径、API密钥、超参数）从代码中剥离，并创建一个工厂来根据配置动态生成组件实例。
*   **关键步骤**:
    1.  创建 `core/config.py`，使用 `Pydantic BaseSettings` 从 `.env` 文件加载配置。
    2.  创建 `core/components_factory.py`，它能根据 `config.py` 中的设置（如 `SM_EMBEDDER_TYPE`），返回一个具体的 `BaseEmbedder` 实现类的实例。
*   **产出物**: `config.py` 配置文件；`components_factory.py` 组件工厂；更新后的 `.env.example`。

**任务5: 实现模型与向量存储组件 (Implement Model & Vector Store Components)**
*   **目标**: 在规范的目录结构中，为核心抽象接口提供具体的实现类，包括调用本地私有化模型和云端API。
*   **关键步骤**:
    1.  创建 `service/core/implementations` 目录，用于存放所有具体的实现类。
    2.  在该目录下，按组件类型创建子目录，如 `embedders`, `rerankers`, `llms`, `vector_stores`。
    3.  在各自的子目录中，创建具体的实现类文件，例如 `implementations/embedders/local_bge.py` 和 `implementations/embedders/dashscope.py`，这些类都继承自 `abstractions` 中对应的基类。
*   **产出物**: 一个组织清晰的 `service/core/implementations` 目录结构；多个具体的组件实现类（如 `LocalBgeEmbedder`, `DashScopeEmbedder`, `FaissVectorStore` 等）；`requirements.txt` 增补。

---

#### 第四部分：服务集成与重构 (Service Integration & Refactoring)

*本部分是重构的收尾阶段，我们将用新的架构组装业务逻辑，并彻底清理旧代码。*

**任务6: 封装统一服务层 (Encapsulate with a Unified Service Layer)**
*   **目标**: 避免在API层处理复杂的业务逻辑，创建一个高内聚的 `RAGService` 来封装和编排所有底层组件的调用流程。
*   **关键步骤**:
    1.  创建 `service/rag_service.py`。
    2.  `RAGService` 的构造函数接收由组件工厂创建的 `Embedder`, `Reranker`, `LLM`, `VectorStore` 等实例。
    3.  在 `RAGService` 内部实现完整的 RAG 问答流程方法（如 `ask`）。
    4.  **【新增】** 在服务层统一构造符合业务需求的 Prompt 模板（例如，包含引用标注规则），并传递给 `BaseLLM` 实例，以确保 Prompt 的一致性和可维护性。
    5.  **【新增】** 在服务层根据业务逻辑（如会话ID、知识库ID）决定要操作的 Elasticsearch 索引名称，并将其作为参数传递给 `BaseVectorStore` 的方法调用（需要先为接口增加 `index_name` 参数）。
*   **产出物**: `RAGService` 类。

**任务7: 重构API层 (Refactor API Layer)**
*   **目标**: 使 `router` 层变得轻薄、职责单一，只负责HTTP请求的解析和响应的格式化。
*   **关键步骤**:
    1.  创建 `dependencies.py`，提供 `get_rag_service` 等依赖项提供函数。
    2.  重构 `router/chat_rt.py` 等API路由，使其通过 `FastAPI.Depends(get_rag_service)` 获取 `RAGService` 实例，并调用其方法来处理业务。
*   **产出物**: 重构后的 `router` 文件；`dependencies.py` 依赖注入模块。

**任务8: 清理遗留代码 (Cleanup Legacy Code)**
*   **目标**: 完成重构的最后一步，移除所有被新架构替代的旧代码，消除技术债务。
*   **关键步骤**:
    1.  审查 `Dealer`, `chat.py`, `retrieval.py` 等旧模块，删除其中与新架构重复或冲突的逻辑。
    2.  彻底移除 `model.py` 中 `generate_embedding` 等与全局状态耦合的旧函数。
*   **产出物**: 一个代码更整洁、结构更清晰的项目。

---

## 第二阶段：核心功能实现——打造科研助理的核心能力 (Phase 2: Core Feature Implementation)

**目标：** 在坚实的架构之上，开发真正服务于“学者”场景的核心功能，让项目从一个通用RAG框架进化为专业的科研助理。

**核心价值：** 展示您将技术与特定业务场景结合的落地能力，以及对前沿技术（多模态、高级检索）的跟进和实践能力。

### **开发流程总览 (Development Workflow Overview)**

为了确保每个功能模块都能高质量、高效率地完成，并与我们第一阶段构建的现代化架构无缝集成，所有Phase 2和Phase 3的功能开发都将遵循以下**标准操作流程 (SOP)**:

### 核心开发理念

*   **小步快跑，原子化提交 (Small, Atomic Commits)**: 每个功能点或修复都应作为一个独立的、完整的提交。这使得代码审查、问题定位和版本回滚变得极其简单高效。
*   **本地优先，容器验证 (Local First, Container Validation)**: 代码的编写、静态检查和单元测试应在本地开发环境（Conda/Venv）中完成，以获得最快的反馈。Docker容器主要用于最终的功能集成验证和确保环境一致性。

### 详细操作流程

1.  **前期准备：配置本地开发环境 (Prerequisites: Local Environment Setup)**
    *   **目标**: 建立一个能提供即时反馈的本地开发环境。
    *   **行动 (一次性)**:
        *   确保本地已安装与 Docker 环境版本一致的 Python。
        *   创建并激活专门用于本项目的 Conda (或 venv) 虚拟环境。
        *   在虚拟环境中 `pip install -r backend/app/requirements.txt` 安装所有依赖。
        *   **配置IDE (VS Code)**: 将IDE的Python解释器指向此虚拟环境，并安装配置 `flake8`, `mypy`, `black`, `isort` 等插件，开启保存时自动格式化和检查。

2.  **需求分析与接口定义 (Define the Contract)**
    *   **目标**: 明确功能的输入、输出和核心行为。
    *   **行动**: 在动手编码前，首先定义后端需要暴露的API接口（输入参数、返回数据结构）和前端的交互逻辑。这将成为前后端开发的“契约”。

3.  **数据模型与持久化 (Data Modeling & Persistence)**
    *   **目标**: 如果新功能需要存储数据，则定义相应的数据结构。
    *   **行动**:
        *   在 `backend/app/models/` 中定义数据库模型。
        *   在 `backend/app/schemas/` 中定义 Pydantic 数据验证模型。**（注意：遵循顶级绝对导入原则，避免循环依赖）**
        *   使用 Alembic 创建和执行数据库迁移脚本。

4.  **TDD驱动：后端服务层实现 (TDD for Backend Service)**
    *   **目标**: 实现健壮、可靠的业务逻辑。
    *   **行动**:
        *   **先写测试 (Test First)**: 在 `backend/app/tests/` 目录下，为即将开发的服务功能编写单元测试 (`pytest`)。明确定义输入和预期输出。
        *   **实现服务**: 在 `backend/app/service/` 目录下创建或修改服务文件，编写业务逻辑代码，直到所有单元测试通过。
        *   **本地验证**: 在本地终端（激活虚拟环境）中反复运行 `pytest`，确保逻辑正确性。
        *   **静态检查**: 在本地终端运行 `flake8 .` 和 `mypy .`，确保代码质量和类型安全。

5.  **API路由层接入 (API Routing)**
    *   **目标**: 将后端服务连接到HTTP接口。
    *   **行动**: 在 `backend/app/router/` 目录下创建新的路由文件，使用 FastAPI 的 `Depends` 将服务注入到API端点中，保持路由层的“轻”和“薄”。

6.  **集成验证 (Integration Validation)**
    *   **目标**: 确保在真实运行环境中功能表现正常。
    *   **行动**:
        *   启动 Docker 环境: `docker-compose up`。
        *   通过 API 工具 (如 Postman, Insomnia) 或前端页面，对开发好的API端点进行实际调用测试，观察容器日志，验证端到端的流程。

7.  **前端组件与集成 (Frontend Implementation)**
    *   **目标**: 构建用户界面并与后端API联调。
    *   **行动**:
        *   在 `frontend/src/pages/` 或 `frontend/src/components/` 中创建新的页面或组件。
        *   在 `frontend/src/api/` 中添加新的API请求函数。
        *   实现用户交互逻辑，并确保UI/UX的流畅性。

6.  **评估与测试 (Evaluation & Testing)**
    *   **目标**: 保证功能的正确性和性能。
    *   **行动**:
        *   编写单元测试和集成测试。
        *   对于RAG相关功能，使用 `evaluation.py` 脚本和 `RAGAs` 等框架进行量化评估，确保核心指标（如忠实度、答案相关性）得到提升或不下降。

7.  **文档更新 (Documentation)**
    *   **目标**: 保持项目文档的同步。
    *   **行动**: 更新 `README.md` 或其他相关文档，说明新功能的使用方法和设计思路。

---

### 功能模块一：智能知识库构建 (Intelligent Knowledge Base Construction)

**功能描述：** 这是整个科研工作流的起点。用户可以围绕一个研究主题（如“GNN+DRL+Edge Inference”），通过与 `ScholarMind` 的交互，从海量文献中筛选、构建出一个高度相关的、私有化的本地知识库。

*   **任务 2.1: 统一的知识库构建 (Unified Knowledge Base Construction)**
    *   **第一部分：需求分析 (Part 1: Requirement Analysis)**
        *   **A. 在线智能导入 (Online Smart Import)**
            *   **用户故事 (User Story):**
                *   “作为一名研究生，当我要开始一个新课题（如‘GNN+DRL+Edge Inference’）的研究时，我希望能创建一个专属的知识库，并让 `ScholarMind` 自动帮我从权威渠道（如Semantic Scholar）检索、筛选出近5年内发表在顶级会议和期刊（如CCF-B类以上，SCI二区以上）的100篇相关论文。我可以在系统推荐的列表中进行最终确认，然后系统会自动下载PDF、提取元数据，为我完成知识库的冷启动，让我能立刻开始高效的文献阅读和问答，而不是淹没在手动搜索和整理文献的繁琐工作中。”
            *   **功能性需求 (Functional Requirements):**
                *   **FR1 - 知识库管理 (Knowledge Base Management)**: 用户可以创建、命名和管理多个独立的知识库。每个知识库与一个用户关联。
                *   **FR2 - 智能导入入口 (Smart Import Entrypoint)**: 在每个知识库的管理界面，提供一个“在线智能导入”的功能入口。
                *   **FR3 - 参数化检索 (Parameterized Search)**: 用户可以输入核心关键词、自定义期望的论文数量上限（例如默认100篇，可调）、以及年份范围来进行检索。
                *   **FR4 - 质量筛选 (Quality Filtering)**:
                    *   **核心挑战**: 第三方学术API通常不直接提供CCF排名或SCI分区这类元数据，无法直接作为查询参数进行过滤。
                    *   **务实方案 (V1)**: 我们将采用“**数据获取 + 后处理辅助决策**”的策略。
                        1.  **数据源**: 优先使用 **Semantic Scholar** API，因为它聚合了来自多个来源的数据，并提供了更丰富的元数据，包括发表会议/期刊名称（venue）。
                        2.  **辅助决策**: 系统内置一份核心的高质量会议/期刊列表（如CCF-A/B）。在返回的检索结果中，如果论文的发表源匹配此列表，则进行**特殊高亮**或标记，从而辅助用户进行手动筛选。**（注意：此为辅助决策，而非强制过滤。未匹配的论文仍会正常显示。）**
                *   **FR5 - 人机协同筛选 (Human-in-the-loop Review)**: 检索完成后，向用户展示一个清晰的列表，包含论文标题、作者、年份、会议/期刊、摘要。用户可以手动勾选确认需要导入的论文。**为提升筛选效率，每篇论文旁应提供一个“查看原文”的链接，指向其在 Semantic Scholar 或源网站上的页面。**
                *   **FR6 - 自动化后台入库 (Automated Background Ingestion)**: 用户确认后，系统启动一个**后台任务**。该任务负责：
                    *   尝试通过开放获取链接（如ArXiv）下载选中论文的PDF。
                    *   将成功下载的文件存入与该知识库关联的专属文件目录中。
                *   **FR7 - 元数据持久化 (Metadata Persistence)**: 每篇成功入库的论文，其详细元数据（标题、作者、摘要、年份、期刊、DOI、Semantic Scholar ID、本地PDF路径等）都必须完整地存入数据库，并与当前知识库建立关联。
                *   **FR8 - 重复文档处理 (Duplicate Document Handling)**: 在导入新论文时，系统应能通过唯一标识（如 `Semantic Scholar ID` 或 `DOI`）检测该文献是否已存在于当前知识库中。如果已存在，则应跳过该文献的下载和处理，并在最终的报告中告知用户。
                *   **FR9 - 清晰的空状态与错误提示 (Clear Empty/Error States)**: 
                    *   当检索结果为空时，前端界面应明确提示“未找到相关文献，请尝试更换关键词”。
                    *   当第三方API请求失败时，应向用户显示一个友好的错误提示，如“文献服务暂不可用，请稍后再试”，而不是直接崩溃或显示技术性的错误信息。
                *   **FR10 - 异步任务管理 (Asynchronous Task Management)**:
                    *   用户的导入请求应该被创建为一个持久化的后台任务（Job），即使前端断开连接，后端也应继续执行。
                    *   系统应提供一个“任务中心”或“历史记录”的界面，用户可以在这里查看所有历史导入任务的状态（排队中、进行中、已完成、部分失败、完全失败）以及详细的处理结果报告。
        *   **B. 本地文件上传 (Local Document Upload)**
            *   **用户故事 (User Story):**
                *   “作为一名研究者，我的电脑里已经用文件夹整理了大量下载好的文献和报告。我希望能直接将这些文件（如PDF, DOCX）批量上传到 `ScholarMind` 的某个知识库中，让系统自动解析、索引它们，并纳入问答范围，使其与我其他的文献资料无缝集成。”
            *   **功能性需求 (Functional Requirements):**
                *   **FR1 - 文件上传界面 (File Upload Interface)**: 在知识库管理页面，提供一个清晰的上传区域（如拖拽区域或选择文件按钮），并支持用户一次性选择多个文件进行批量上传。
                *   **FR2 - 支持的文件类型 (Supported File Types)**: 初期核心支持学术研究中最常见的文件格式：`.pdf`, `.docx`, `.txt`。系统架构应易于未来扩展，以支持更多格式（如 `.md`, `.html`）。
                *   **FR3 - 后台异步处理 (Background Processing)**: 文件上传成功后，应触发一个后台任务。该任务将复用统一的文档处理流水线，负责对文件进行**解析、内容提取、分块（Chunking）、向量化、并最终存入向量数据库**。
                *   **FR4 - 元数据提取 (Metadata Extraction)**: 系统应尽力从上传的文件中提取元数据。例如，对于PDF，尝试解析其标题、作者等信息；如果无法提取，则默认使用文件名作为文档标题。
                -   **FR5 - 数据持久化 (Data Persistence)**:
                    *   将解析后的文档文本内容、元数据存入 `documents` 表。
                    *   将原始文件存放在结构化的文件存储路径中（遵循 **NFR5** 规则）。
                *   **FR6 - 任务管理与报告 (Task Management & Reporting)**: 上传任务同样应纳入“任务中心”，用户可以跟踪处理进度（排队中、处理中、完成、失败），并查看最终报告（例如，成功 N 个，失败 M 个及原因）。
                *   **FR7 - 重复文件检测 (Duplicate File Detection)**: 在同一个知识库中，系统应能通过文件的哈希值（如 SHA256）来检测并阻止内容完全相同的文件被重复上传。
        *   **C. 文档管理 (Document Management)**
            *   **用户故事 (User Story):**
                *   “我的知识库已经导入了几十篇文献，但其中一些本地上传的文档标题是文件名，我想改成真正的论文标题，并补充作者信息。同时，我发现有几篇文献质量不高，希望能从知识库里彻底删除它们，以保证问答的精准性。”
            *   **功能性需求 (Functional Requirements):**
                *   **FR1 - 文档列表视图 (Document List View)**: 在每个知识库的管理界面，提供一个包含所有已导入文档的列表，并支持搜索和排序。
                *   **FR2 - 元数据编辑 (Metadata Editing)**: 用户可以点击列表中的任意文档，打开一个表单来编辑其所有元数据字段（如标题、作者、年份、摘要、关键词等）。
                *   **FR3 - 文档删除 (Document Deletion)**: 用户可以从知识库中选择一个或多个文档进行删除。该操作应是彻底的，需要同步删除：
                    1.  数据库中的元数据记录。
                    2.  向量存储中关联的文档区块（Chunks）。
                    3.  服务器上存储的原始文件。
        *   **共享的非功能性需求 (Shared Non-Functional Requirements):**
            *   **NFR1 - 响应性 (Responsiveness)**: 检索和下载/上传过程不能阻塞UI。整个流程需要有明确的进度提示（如：正在检索、等待用户确认、正在后台下载 X/Y 篇...、文件上传进度...）。
            *   **NFR2 - 稳定性与容错 (Stability & Fault Tolerance)**: 后台处理任务应是稳定的。单篇论文的处理失败（如找不到PDF源、文件损坏）不能中断整个批次。任务结束后，应向用户提供一份清晰的处理结果报告（成功N篇，失败M篇及原因）。
            *   **NFR3 - API韧性 (API Resilience)**: 与第三方学术API（如 Semantic Scholar）交互时，必须考虑到其速率限制（Rate Limiting）。`ImporterService` 应实现优雅的重试机制（如指数退避），以避免因请求过于频繁而被封禁。
            *   **NFR4 - 权限控制 (Authorization)**: 所有与特定知识库相关的API端点（如 `/knowledgebases/{kb_id}/...`）必须进行严格的权限校验，确保当前操作的用户是该知识库的所有者。
            *   **NFR5 - 结构化文件存储 (Structured File Storage)**: 明确定义文件存储的路径规则，例如：`{STORAGE_ROOT}/{user_id}/{knowledge_base_id}/{document_id}.pdf`。这种结构清晰、隔离性好，便于未来的权限管理和数据备份。
            *   **NFR6 - 操作原子性 (Atomicity of Operations)**: 入库操作应具备事务性。只有当“文件落盘”和“元数据入库”两个步骤全部成功时，该次操作才算最终成功。如果任何一步失败，系统应有能力回滚已完成的步骤（例如，删除已保存的文件），以保证数据的一致性。
            *   **NFR7 - 安全性 (Security)**: 必须在客户端和服务器端对上传文件的类型和大小进行校验，防止恶意文件上传。

            
    *   **第二部分：实施计划 (Part 2: Implementation Plan)**
        *   **数据模型 (Data Models):**
            *   创建 `KnowledgeBase` 模型 (在 `models/knowledgebase.py` 中)，包含 `id`, `name`, `user_id` 等字段。
            *   创建 `Document` 模型 (可以复用或增强 `models/document_upload.py`)，包含所有元数据字段，并与 `KnowledgeBase` 关联。
        *   **后端 (Backend):**
            *   创建 `KnowledgeBaseService` (在 `service/knowledgebase_service.py` 中)，负责知识库的增删改查逻辑。
            *   创建 `ImporterService` (在 `service/importer_service.py` 中，例如 `SemanticScholarImporter`)，封装第三方API的调用、结果解析、质量筛选标记和后台下载逻辑。
        *   **API:**
            *   在 `router` 目录下创建 `knowledgebase_rt.py`。
            *   定义API端点，如 `POST /knowledgebases` (创建), `GET /knowledgebases` (列表), `POST /knowledgebases/{kb_id}/import/search` (搜索), `POST /knowledgebases/{kb_id}/import/run` (执行导入)。
        *   **前端 (Frontend):**
            *   改造知识库管理页面 (`/repository`) 以支持多知识库。
            *   开发“在线导入”的完整UI流程，包括搜索表单、带高亮标记的结果列表、确认按钮和后台任务状态的实时显示。

*   **任务 2.2: 重构遗留核心模型以适配多知识库架构 (Refactor Legacy Models for Multi-KB Architecture)**
    *   **核心动机**: 原有的 `Session` 和 `Message` 模型是为单文档、临时会话设计的。为支持在指定知识库内进行对话的核心功能，必须将这些模型与 `KnowledgeBase` 关联起来，这是承上启下的关键架构演进。
    *   **实施计划**:
        1.  **(待办)** **重构 `Session` 模型**:
            *   增加一个 `knowledge_base_id` 字段（外键关联到 `knowledge_bases.id`），明确每个聊天会话所属的知识库。
        2.  **(待办)** **重构 `Message` 模型**:
            *   确认其与 `Session` 的关联是否依然稳固，并评估是否需要冗余 `knowledge_base_id` 以优化查询。
        3.  **(待办)** **重新评估 `DocumentUpload` 模型**:
            *   明确其在新架构下的定位。例如，所有“快速上传”的文件都自动进入一个用户的默认知识库。
        4.  **(待办)** **更新所有相关的 Pydantic Schemas** 以反映数据模型的变更。
        5.  **(待办)** **创建并应用新的数据库迁移 (Alembic)**。
    *   **产出物**: 与知识库兼容的会话和消息数据模型；更新后的数据库结构。

*   **任务 2.3: 多模态内容解析 (Multi-modal Ingestion)**
        *   **现状分析**: 原项目的 `PdfParser` 具备提取图片的能力，但这些宝贵的多模态信息在上层被直接丢弃了。
        *   **描述**: 增强文档解析流程，在识别文本的同时，使用 `PyMuPDF` 等工具提取PDF中的**图片、表格、公式**。调用一个本地部署的多模态模型（如 `LLaVA` 或 `Qwen-VL`）对图表生成详细的文字描述（Image Captioning）。将这些结构化信息（图片路径、图表描述、公式LaTex）与文本块一同存入Elasticsearch。
    *   **实施计划**:
        1.  **架构**: 定义 `BaseImageCaptioner` 抽象类。
        2.  **实现**: 实现一个具体的 `LocalVLCaptioner` 类。
        3.  **集成**: 修改 `document_operations.py` 中的文件解析逻辑，在解析时调用 `ImageCaptioner` 服务，并将生成的描述和图片信息存入 `Chunk` 模型的 `metadata` 字段。
        4.  **数据**: 更新Elasticsearch的索引映射（mapping）以支持存储这些新的元数据。
        5.  **产出物**: 增强的文档解析服务；包含图文描述的、更丰富的索引结构。

*   **任务 2.4: 语义感知分块 (Semantic-Aware Chunking)**
        *   **核心动机**: 解决按固定Token数切块，经常**将一句完整的语义从中间切断**，从而破坏上下文完整性的问题。
        *   **描述**: 放弃按固定Token数切块的简单策略。引入 `SentenceTransformers` 库，通过计算句子间向量的语义相似度来决定切分点，确保每个文本块都是一个语义完整的“意群”。
    *   **实施计划**:
        1.  **架构**: 定义 `BaseChunker` 抽象类。
        2.  **实现**: 创建一个 `SemanticChunker` 实现类，封装基于语义相似度的切分逻辑。
        3.  **集成**: 在 `document_operations.py` 中，用 `SemanticChunker` 替换掉原有的简单切分逻辑。
        4.  **产出物**: 一个基于语义的 `SemanticChunker` 工具类。

### 功能模块二：多文档、高精度 RAG 问答 (High-Fidelity, Multi-Document RAG Q&A)

**功能描述：** 这是 `ScholarMind` 的核心交互功能。用户可以在构建好的知识库范围内，用自然语言进行提问。系统不仅能给出精准的答案，还必须提供清晰、可验证的答案来源，并支持在一个对话中围绕多篇论文进行持续的、有上下文的讨论。

*   **任务 2.5: 基础能力：多文档会话 (Multi-Document Session Support)**
        *   **核心动机**: 解决原项目 `quick_parse` 只能“一问一答一文档”的局限性，使其不符合用户直觉，且无法满足多文档对比等核心需求。
        *   **描述**: 重构 `chat` 相关的服务和API，使单个聊天会话（Session）可以关联一个或多个知识库文档，允许用户在一个对话中持续就多篇论文进行提问和比较。
    *   **实施计划**:
        1.  **数据模型**: 修改数据库模型，建立 `Session` 与 `Document` 之间的多对多关系。
        2.  **后端**: 修改 `chat_rt.py` 的API，允许在创建或选择会话时关联多个文档ID。
        3.  **后端**: 修改 `RAGService`，使其能从与当前会话关联的多个文档中检索上下文。
        4.  **前端**: 改造聊天界面，允许用户选择或查看当前会话关联了哪些文档。
        5.  **产出物**: 支持多文档上下文的聊天API和后端服务逻辑。

*   **任务 2.6: <font color='red'>高级检索策略 (Advanced Retrieval Strategies)</font>**
        *   **描述**: 实现至少一种高级检索策略来提升召回精度，确保找到最相关的上下文。
            *   **首选**: **多路召回 (Multi-Query Retrieval)**。用LLM将用户的原始问题分解为3-5个不同角度的子问题，并行检索，最后用**RRF (Reciprocal Rank Fusion) 算法**高效地合并结果。
            *   **备选**: **假设性文档嵌入 (HyDE)**。先让LLM为问题“幻想”一个答案，再用这个幻想答案的向量去检索。
    *   **实施计划 (以多路召回为例)**:
        1.  **架构**: 定义 `BaseRetriever` 抽象类，它封装了从查询到获取 `Chunk` 列表的整个过程。
        2.  **实现**: 创建 `VectorStoreRetriever` (封装基础的向量搜索) 和 `MultiQueryRetriever`。`MultiQueryRetriever` 内部会调用LLM生成子问题，然后并行调用 `VectorStoreRetriever`，最后用RRF算法聚合结果。
        3.  **集成**: 修改 `RAGService`，将直接调用 `vector_store.search` 的逻辑替换为调用 `retriever.retrieve`。
        4.  **评估**: 在 `evaluation.py` 中添加A/B测试，用数据证明 `MultiQueryRetriever` 相对于基线 `VectorStoreRetriever` 的性能提升。
        5.  **产出物**: 可配置的查询扩展模块与RRF实现；A/B评估记录。

*   **任务 2.7: 答案溯源与高亮 (Citation & Highlighting)**
        *   **描述**: 在生成答案后，必须精确地展示每个论点来源于哪篇论文的哪个部分。将引用粒度细化到“文档ID-页码-段落/坐标”，返回结构化的 `citation` 数据。前端可以根据这些数据，在答案旁边展示引用的原文片段，并提供一个链接，点击后可以直接打开对应的PDF文件并高亮显示相关区域。
    *   **实施计划**:
        1.  **后端**: 增强 `RAGService`，使其在调用LLM后，对生成的内容进行后处理，验证答案中的每个句子是否能被召回的上下文支持，并记录下精确的引用来源（`Chunk`中的`metadata`）。
        2.  **API**: 修改聊天API的返回结构，增加一个 `citations` 字段，包含结构化的引用信息。
        3.  **前端**: 开发一个引用展示组件。当答案显示时，解析 `citations` 数据，在旁边显示引用的原文片段，并提供可点击的链接。
        4.  **产出物**: `post_generation_validator` 模块；标准化的引用数据结构；前端高亮示例。

*   **任务 2.8: 【亮点功能】<font color='red'>跨论文深度对比分析 (Cross-Paper Comparative Analysis)</font>**
        *   **功能描述**: 支持用户指定2-3篇论文，提出结构化的对比问题，如“**用表格对比论文A和B在方法论、数据集和性能指标上的异同**”。
    *   **实施计划**:
        1.  **后端**: 在 `RAGService` 中新增一个 `compare_papers` 方法。
        2.  **逻辑**: 该方法接收论文ID列表和对比要求，构建一个特殊的、带有结构化输出指令（如 "以Markdown表格形式输出"）的Prompt。
        3.  **API**: 创建一个新的API端点 `/chat/compare`。
        4.  **技术价值**: 这要求系统具备更强的结构化信息提取和逻辑推理能力，是高阶的RAG应用。

*   **任务 2.9: 【亮点功能】<font color='red'>批判性问题生成 (Critical Question Generator)</font>**
        *   **功能描述**: 在用户读完一篇论文后，`ScholarMind` 能主动提出批判性问题，如“**分析论文A的作者在实验部分规避了哪些潜在的负面结果？**”，引导用户深度思考。
    *   **实施计划**:
        1.  **后端**: 在 `RAGService` 中新增 `generate_critical_questions` 方法。
        2.  **逻辑**: 该方法接收一个文档ID，检索其核心内容（如摘要和结论），然后调用LLM，使用一个引导批判性思维的Meta-Prompt来生成问题。
        3.  **API**: 创建一个新的API端点 `/document/{doc_id}/critical_questions`。
        4.  **技术价值**: 要求LLM具备反思（Reflection）和批判性思维能力，让项目从“工具”变为“伙伴”。

*   **任务 2.10: <font color='red'>【核心技术：RL微调】自适应重排序模型 (Adaptive Reranker via RL)</font>**
        *   **动机**: 传统的Reranker微调依赖静态数据集，无法适应用户的动态偏好。通过强化学习，我们可以利用用户的**隐式反馈**（如点击、滚动）作为奖励信号，让 Reranker 动态地、持续地优化其排序策略，变得越来越懂用户。
    *   **实施计划 (奠定基础)**:
        1.  **数据收集**: 修改前端的引用展示组件和后端的API，当用户点击某个引用来源时，记录下这次点击事件（哪个查询，哪个被点击的文档）。这是未来RL训练的**核心奖励数据**。
        2.  **架构准备**: 我们已经定义了 `BaseReranker` 抽象。这个设计使得未来可以无缝插入一个 `RLFineTunedReranker` 实现，而无需改动 `RAGService` 的主体逻辑。
        *   **RL建模**:
            *   **状态 (State)**: 用户查询 `Query` 和初步召回的 `Documents` 列表。
            *   **动作 (Action)**: Reranker 对 `Documents` 列表进行**重新排序**。
            *   **奖励 (Reward)**: 当用户在最终答案的引用中**点击**了某个来源，或对某个来源点了“赞”，就给予该排序动作**正奖励**；反之则给予**负奖励**。
        *   **产出物**: 一个通过在线学习持续进化的 Reranker 模型；一套用户反馈收集与奖励计算机制；一份展示模型自适应优化过程的技术报告，这是体现 **RL+RAG** 结合深度的绝佳案例。

---

## 第三阶段：迈向智能助理——实现自动化与深度优化 (Phase 3: Towards an Intelligent Agent & Deep Optimization)

**目标：** 让 `ScholarMind` 从一个“被动问答”的工具，升级为一个能“**主动执行复杂任务**”的智能助理，构筑项目的核心技术壁垒。

**核心价值：** 这是您冲击高级岗位或顶级公司的“大杀器”。它证明您不仅是“功能实现者”，更是具备“系统优化”和“AI应用创新”能力的稀缺人才。

### 功能模块三：AI 辅助写作 (AI-Powered Writing Assistance)

**功能描述：** 针对论文写作中最耗时的部分，提供智能化的辅助。`ScholarMind` 可以帮助用户润色语言、根据知识库内容生成论文片段、甚至自动化完成初稿。

*   **任务 3.1: 学术语言润色与基于知识库的片段写作**
    *   **描述**:
        *   **润色**: 提供一个“润色”功能。用户输入一段较为口语化的草稿，LLM 会将其改写为更符合学术规范、更书面化的语言。
        *   **片段写作**: 用户指定一个主题和写作要求（如“帮我写一段关于GNN在交通流量预测中应用的介绍”），系统首先在知识库中进行RAG检索，然后将检索到的上下文组织起来，喂给LLM生成符合要求的段落。
    *   **实施计划**:
        1.  **后端**: 创建 `writing_service.py`。
        2.  **后端**: 在服务中实现 `polish_text` 和 `generate_snippet` 方法。后者会调用 `RAGService` 来获取相关上下文。
        3.  **API**: 创建 `/writing/polish` 和 `/writing/generate_snippet` 两个API端点。
        4.  **前端**: 创建一个新的“写作助手”页面。

*   **任务 3.2: 【亮点功能】<font color='red'>智能引用推荐 (Smart Citation Suggestion)</font>**
        *   **功能描述**: 这是一个强大的“**边写边引 (Cite-as-you-write)**”功能。用户在撰写论文时，可以输入一段自己写好的、但尚未添加引用的段落。`ScholarMind` 会智能分析这段话的核心论点，自动在本地知识库中检索支撑这些论点的文献，并将标准的引用标记（如 `[1]`, `(Author, Year)`）插入到文本的适当位置，同时在文末生成对应的参考文献列表。
        *   **技术实现路径**:
        1.  **论点切分**: 在 `writing_service.py` 中，使用 NLP 技术或 LLM 将用户输入的段落切分成多个独立的语义论点或句子。
        2.  **批量检索**: 对每一个论点，调用 `RAGService` 的检索模块，在知识库中找到最相关的 Top-K 篇文献或段落。
            3.  **引用决策**: 让 LLM 判断检索到的文献是否确实能支撑原始论点。如果能，则确定引用该文献。
            4.  **文本注入与格式化**: 在原始文本的论点句末尾插入引用标记，并调用独立的格式化工具生成参考文献列表。
        *   **产出物**: 一个新的 API 端点 `/writing/suggest_citations`，接收文本块，返回带引用的文本块和参考文献列表。

### <font color='red'>功能模块四：自动化 Related Work 撰写 (Agent-Powered Related Work Generation)</font>

**功能描述：** 这是 `ScholarMind` 的“杀手级”功能，是 **Agent 技术**的最佳应用场景。用户只需给出一个主题，Agent 就能**自主地在知识库中进行检索、阅读、提炼、总结**，并最终生成一份结构清晰、逻辑连贯的“Related Work”章节初稿，并自动整理好参考文献列表。

*   **任务 3.3: Agent化与工具调用 (Agentic Workflow with Tool Calling)**
        *   **描述**: 引入**工具调用 (Tool Calling)** 思想，让LLM能自主规划并调用您提供的工具集来完成复杂任务。
    *   **实施计划**:
        1.  **架构**: 定义一个 `BaseTool` 接口，并创建 `agent_service.py`。
        2.  **工具集实现**: 将现有的服务功能封装成 Agent 可调用的工具。
            *   `search_papers_tool`: 封装 `RAGService` 的检索功能。
            *   `read_and_summarize_tool`: 封装 `RAGService`，对指定论文ID列表进行内容总结。
            *   `synthesize_related_work_tool`: 封装一个LLM调用，用于整合多篇摘要。
            *   `format_bibliography_tool`: 封装一个参考文献格式化工具。
        3.  **Agent执行器**: 在 `agent_service.py` 中，实现一个循环逻辑（ReAct 模式）。该逻辑根据用户目标，驱动LLM选择工具、执行、观察结果，直到任务完成。
        4.  **API**: 创建 `/agent/generate_related_work` 端点。
    *   **产出物**: 一个具备自主规划能力的 Related Work 撰写 Agent 服务。

### <font color='red'>【亮点功能】功能模块五：知识库可视化分析 (Knowledge Base Visualization & Analysis)</font>

**功能描述：** 提供一个“上帝视角”来洞察整个知识库。系统可以自动从所有论文中抽取出作者、技术、论文之间的复杂关系，并以**交互式知识图谱**的方式呈现给用户。用户可以直观地看到哪些作者是领域核心，哪些技术是研究热点，以及不同研究方向之间的关联。

*   **任务 3.4: 基于LLM的知识图谱构建与可视化**
    *   **实施计划**:
        1.  **后端 (批量处理)**: 创建一个离线脚本 `knowledge_graph_builder.py`。该脚本遍历知识库中的所有文档，调用LLM进行信息抽取（实体、关系），并将结果存为结构化数据（如JSON或存入图数据库）。
        2.  **后端 (API)**: 创建 `graph_service.py` 和相应的API端点 `/graph/knowledge`，用于查询和提供知识图谱数据给前端。
        3.  **前端**: 创建一个新的可视化页面，使用 `ECharts`, `D3.js` 或 `Vis.js` 等图表库，调用后端API获取数据并渲染成可交互的关系图。
    *   **产出物**: `knowledge_graph_builder.py` 脚本；一个新的前端可视化页面和组件。

### 支撑模块：系统优化与评估 (Supporting Modules: Optimization & Evaluation)

**描述：** 为了确保上述所有功能的专业性和可靠性，我们需要建立一套科学的评估和优化体系，并探索最前沿的模型优化技术。

*   **任务 3.5: 构建量化评估体系 (RAG Evaluation)**
        *   **描述**: 集成 `RAGAs` 或 `TruLens` 评估框架，构建一个小的评测数据集，用以量化计算系统的**忠实度 (Faithfulness)、答案相关性 (Answer Relevancy)、上下文精确率/召回率 (Context Precision/Recall)**等核心指标。
    *   **实施计划**:
        1.  **数据集**: 创建一个 `evaluation/dataset.jsonl` 文件，包含一系列问题、理想答案和相关上下文文档。
        2.  **脚本**: 创建 `evaluation/run_eval.py` 脚本。该脚本加载数据集，调用 `RAGService` 运行评估，并生成报告。
        3.  **CI/CD (可选)**: 将评估脚本集成到CI/CD流程中，确保每次代码合并都不会导致核心指标下降。
        *   **产出物**: 一个 `evaluation.py` 脚本；一份 `evaluation_report.md`，用数据驱动每一次优化。

*   **任务 3.6: <font color='red'>【终极目标：RL微调】Agent 规划策略优化 (Agent Planning Policy Optimization via RL)</font>**
        *   **动机**: Agent 完成复杂任务（如撰写Related Work）依赖于一系列工具调用的“决策链”。一个优秀的 Agent 不仅要“会用”工具，更要学会“**聪明地、高效地**”使用工具。通过强化学习，我们可以**优化 Agent 的规划与推理策略本身**。
    *   **实施计划 (奠定基础)**:
        1.  **数据收集**: 在 `agent_service.py` 中，详细记录 Agent 的每一步决策（思考过程、工具选择、参数），以及最终产出的质量（可由更高阶的LLM如GPT-4o或用户打分）。这是未来RL训练的核心轨迹数据。
        2.  **架构准备**: 我们模块化的 `agent_service.py` 和 `BaseTool` 抽象，使得未来可以方便地替换掉Agent的核心LLM为一个经过RL策略优化的新模型。
        *   **RL建模**:
            *   **状态 (State)**: 当前的任务目标和已经收集到的信息。
            *   **动作 (Action)**: Agent 在当前状态下，选择调用哪个**工具**以及传入什么**参数**。
            *   **奖励 (Reward)**: 这是一个**稀疏奖励**场景。只有当 Agent 完成所有步骤生成最终报告后，才能由一个更高阶的模型（如GPT-4o）或用户来给最终产出一个综合质量评分，作为对整个决策链的**总奖励**。
        *   **产出物**: 一个经过 PPO/REINFORCE 算法微调、具备更优规划能力的 LLM Agent；一份阐述如何优化 LLM Agent 推理过程的顶级技术报告，**完美契合 RL+LLM Agent 这一前沿求职方向**。

*   **任务 3.7: 全链路可观测性 (Observability) - (推荐)**
        *   **描述**: 集成 `LangSmith` 或自建日志系统，追踪一次复杂任务（尤其是Agent调用）的全链路，方便调试和定位问题。
        *   **产出物**: 可观测性中间件；`tracing.md` 使用说明。

### **量化成功指标 (Quantifiable Success Metrics)**

**描述**: 这里统一存放我们所有核心技术模块的量化目标，将由“构建量化评估体系”模块负责具体评测和生成报告。这些指标是衡量我们项目成功与否的关键，也是写入简历的核心成果。

        *   **具体指标**:
            *   **高级检索策略**: 相较于基线检索，将核心指标 **Context Precision 提升20%**。
            *   **自适应Reranker (RL)**: 将用户对引用来源的**点击率提升30%**。
            *   **智能引用推荐**: 在测试集上达到 **90% 的引用推荐准确率**。
            *   **Agent自动化写作**: 生成的初稿达到**只需少量（~20%）人工修改即可用**的水平。
            *   **知识图谱构建**: 在标注测试集上，实体关系抽取的 **F1 分数达到 85% 以上**。
            *   **Agent策略优化 (RL)**: 将 Agent 生成报告的**平均质量分从 7/10 提升至 9/10**。
            *   **基础RAG评估**: 将系统的 **Faithfulness 从 0.8 提升至 0.95**。
